DOSE: Drum One-Shot Extraction from Music Mixture Suntae Hwang1 Seonghyeon Kang1 Kyungsu Kim1 Semin Ahn2 Kyogu Lee1,3 Abstract—Drum one-shot samples are crucial for music production, particularly in sound design and electronic music. This paper introduces Drum One-Shot Extraction, a task in which the goal is to extract drum one-shots that are present in the music mixture. To facilitate this, we propose the Random Mixture One-shot Dataset (RMOD), comprising large-scale, randomly arranged music mixtures paired with corresponding drum one-shot samples. Our proposed model, Drum One-Shot Extractor (DOSE), leverages neural audio codec language models for end-to-end extraction, bypassing traditional source separation steps. Additionally, we introduce a novel onset loss, designed to encourage accurate prediction of the initial transient of drum one-shots, which is essential for capturing timbral characteristics. We compare this approach against a source separation-based extraction method as a baseline. The results, evaluated using Frechet Audio Distance (FAD) and Multi-Scale ´ Spectral loss (MSS), demonstrate that DOSE, enhanced with onset loss, outperforms the baseline, providing more accurate and higher-quality drum one-shots from music mixtures. The code, model checkpoint, and audio examples are available at https://github.com/HSUNEH/DOSE Index Terms—Drum, One-Shot, Music Source Separation, Neural Audio-Codec, Generative Model I. INTRODUCTION Drum sounds are fundamental components of contemporary music production, playing a crucial role across various genres such as electronic music, hip-hop, and pop. In these genres, music producers often construct rhythmic patterns by sequencing individual drum one-shot samples, allowing for precise control over the timbral and temporal characteristics of the rhythm. Given the significance of drum sounds in music production, there has been substantial research interest in synthesizing drum one-shot samples using advanced technologies, including recent developments in deep learning [1]–[5]. These novel approaches utilize conditional inputs such as drum type or acoustic features, aiming to facilitate intuitive sample creation for music producers without requiring extensive knowledge of signal processing techniques.

In many practical scenarios, music producers work with existing recordings to create remixes, cover versions, or other productions, necessitating the extraction of high-quality drum samples directly from music mixtures. We define this task as Drum One-Shot Extraction. A conventional approach to this task involves applying music source separation techniques to isolate the drum track, followed by This work was partly supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) [No. RS-2023- 00219429, $5 0 \%$ ], Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) [No. RS-2022-II220320, 2022-0-00320, Artificial intelligence research about cross-modal dialogue modeling for one-on-one multi-modal interactions, $4 0 \% ]$ , [No. RS-2021-II212068, Artificial Intelligence Innovation Hub (Artificial Intelligence Institute, Seoul National University), $5 \%$ ], and [NO.RS-2021- II211343, Artificial Intelligence Graduate School Program (Seoul National University), $5 \%$ ] the identification and extraction of segments containing isolated oneshot samples. We refer to this type of method as the “separation-based approach”. However, this approach cannot guarantee the identification of isolated one-shot segments and is dependent on separation algorithms, potentially introducing artifacts and compromising sample quality.

To address these limitations, we propose a novel generationbased one-shot extraction approach that circumvents the intermediate separation step and directly generates drum one-shot samples from the input music mixture. Our method, DOSE, leverages recent advancements in neural audio codec language modeling to perform end-to-end generation of drum one-shots. DOSE employs separate decoder-only Transformers for each drum type (kick, snare, and hihat) to achieve better extraction performance compared to using a single model for all drum types.

The architecture of DOSE closely follows that of MusicGen [6] , utilizing the same core components of neural audio codec and decoder-only transformer. The DAC [7] encoder encodes both the music mixture and the one-shot audio into discrete acoustic tokens. The transformer is then trained to autoregressively generate the acoustic tokens of the one-shot samples conditioned on the acoustic tokens of the music mixture. The generated tokens are decoded into waveform audio via the DAC decoder.

For training and evaluation of DOSE, we introduce the Random Mixture One-shot Dataset (RMOD), a novel paired dataset comprising synthetically generated music mixtures and their corresponding drum one-shot samples. RMOD consists of 360,000 pairs of mixture audio and corresponding drum one-shot samples, created by randomly mixing drum tracks, which are synthesized using one-shot samples, with instrument tracks.

We conducted a comprehensive quantitative evaluation of DOSE and baseline methods on the one-shot drum sample extraction task. In addition to RMOD, we utilized the Groove MIDI Dataset [8] in our evaluation, which offers more realistic drum performances.Our experimental results demonstrate that DOSE outperforms the baseline method, which is a separation-based approach implemented using LarsNet[citation], across various objective metrics, including Frechet Audio Distance (FAD) [9] and Multi-Scale Spectral Similar- ´ ity (MSS)) [10].

II. RELATED WORK Drum One-Shot Generation. Generating drum one-shot samples has received considerable attention with the development of neural audio synthesis methods. Early approaches leveraged models such as Variational Autoencoders (VAE) [11] and Generative Adversarial Networks (GAN) [12], which introduced latent-space exploration and controllable synthesis for drum sounds. Subsequent works like NeuroDrum [2], DrumGAN [4], and StyleWaveGAN [5] further improved controllability and timbral diversity by conditioning on various audio features (e.g., brightness, boominess, depth).

Score-based diffusion models [13] have recently emerged as a powerful paradigm for audio synthesis, offering flexible sampling strategies. CRASH [14], for instance, leverages stochastic differential equations (SDEs) to generate high-resolution percussive sounds $( 4 4 . 1 \mathrm { k H z } )$ in a controllable manner, matching the fidelity of GANbased methods while enabling techniques such as class mixing to create “hybrid” sounds. These advancements illustrate the growing breadth of approaches for drum/percussive sound generation, providing more interactive and fine-grained creative possibilities for music producers.

Drum Source Separation. In traditional music source separation, the goal is to split a mixture into individual instrument stems, including drums, bass, vocals, and others [15]–[18]. More specific to drumoriented tasks, Mezza et al. [19] introduced a new challenge called Drum Source Separation, aiming to decompose a drum track into separate stems (kick, snare, hi-hat, etc). Their proposed method, LarsNet, accomplishes drum-component separation, allowing for individual extraction of each drum type.

Neural Audio Codec Language Modeling. Neural audio codecs such as SoundStream [20], EnCodec [21], and DAC [7] have received notable attention for their ability to compress audio into discrete tokens with minimal perceptual loss. When combined with transformer models, these tokenizers can be leveraged for autoregressive audio generation tasks. MusicGen [6] and MusicLM [22] are prime examples, demonstrating high-quality music generation via codecbased language models.

III. METHOD We propose DOSE, a deep learning model designed to extract drum one-shot sounds (kick, snare, and hi-hat) from complex audio mixtures. Inspired by MusicGen [6], DOSE employs a decoder-only transformer to process discrete audio representations. To emphasize accurate transient predictions, we introduce a novel onset loss during training.

A. Autoregressive Acoustic Token Generation DOSE generates drum one-shot sounds by autoregressively predicting acoustic tokens conditioned on mixture-audio tokens. This involves converting audio waveforms into discrete tokens using Descript Audio Codec (DAC) [7], predicting drum one-shot tokens, and decoding them into waveforms.

DAC processes mono audio input $x \in \mathbb { R } ^ { T _ { \mathrm { i n } } \cdot f _ { s } }$ , tokenizing it into a discrete code where $K$ is the number of codebooks, $N$ is the codebook size, and $f _ { c } \ll f _ { s }$ is the codec frame rate.

The decoder-only transformer autoregressively predicts drum oneshot tokens conditioned on the mixture-audio tokens. The DAC decoder then converts these tokens back to audio waveforms. DOSE is trained separately for each drum type (kick, snare, and hi-hat), following the structure of decoder-only language models [6] and employing the interleaving delay pattern from [23].

B. Training Loss Our training objective combines two cross-entropy losses: (1) fulllength cross-entropy, computed over all predicted tokens in the drum one-shot, and (2) onset loss, emphasizing the attack or transient portion by focusing on tokens with indices $t + k \le K + 1$ , where $t$ is the codec frame index and $k$ is the codebook index. These transient regions strongly influence perceived timbre [24].

The final loss used for training is summation of the full-length loss and the onset loss: This setup biases the model to focus on transient regions, improving the perceptual fidelity of generated one-shot samples.

IV. DATASET Existing drum sound datasets are not well-suited for one-shot drum extraction, as they often lack proper alignment between mixed audio tracks and their corresponding drum one-shot samples (e.g., kick, snare, hi-hat). This limitation makes it challenging for models to learn one-shot drum extraction compared to drum stem separation tasks, for which more established datasets exist.

To address this gap, we developed the Random Mixture One-shot Dataset (RMOD), a large-scale dataset that includes numerous pairs of randomly mixed music loops and the corresponding drum one-shot samples. These pairs serve as the basis for training and evaluating one-shot drum extraction models.

The overall data generation process for RMOD is illustrated in Figure 3. Drum one-shot samples were first used to create drum loops, which were then mixed with instrumental loops such as guitar, piano, and bass to form complete musical mixtures.

For the drum one-shot samples, we leveraged publicly available datasets [25]–[27]. Instrumental loops were sourced from the Logic Pro [28] library. In total, we collected 3,375 kick samples, 1,801 snare samples, 1,278 hi-hat samples, 454 piano samples, 1,161 guitar samples, 1,782 bass samples, and 202 vocal samples. Using augmentation and mixing techniques described in next sections, we generated one million pairs of randomly mixed music mixtures and their corresponding drum one-shot samples for training, with an additional 10,000 pairs each for validation and testing.

The RMOD dataset has been made publicly available through a dedicated repository, along with detailed documentation to facilitate its use by the broader research community.

A. Drum One-Shot Augmentation To increase the diversity of the RMOD dataset and reflect realworld production techniques, we applied drum layering as a data augmentation method.Two drum one-shot samples were randomly selected and layered with amplitude weights of (0.8, 0.2), (0.7, 0.3), and (0.6, 0.4), corresponding to decibel reductions of approximately (-1.94 dB, -13.98 dB), (-3.01 dB, -10.46 dB), and (-4.44 dB, -7.96 dB). This weighted sum approach generated diverse drum combinations. This method effectively expanded the dataset, providing the model with a richer and more diverse set of training examples.

B. Loop Generation Process To efficiently generate a large and diverse dataset, RMOD does not aim to closely mimic the structure of real music tracks. Instead, we employed a process of random mixing to create four-second loops, which allows for the rapid creation of a large number of training samples.

MIDI Generation: We began by creating MIDI (Musical Instrument Digital Interface) note sequences to control the timing and placement of drum sounds within each loop. Using miditoolkit [29] Python library, each four-second loop was divided into 1,920 equally spaced grid points, with kick drum sounds randomly occurring 2 to 4 times, snare drums 2 to 4 times, and hi-hats 14 to 18 times. This random variability in timing and placement provided the model with a wide range of drum sequences, improving its generalization to real-world scenarios.

Audio Rendering from MIDI: Once the MIDI sequences were created, each note was mapped to a corresponding drum one-shot sample from RMOD. If a new onset occurred for the same drum class while a previous sample was still playing, the earlier sound was sliced to prevent overlap within that class. This behavior reflects real-world scenarios, such as rapid hi-hat strikes, and does not affect overlapping sounds from different drum classes. For instrumental tracks, we used guitar, piano, bass, and vocal loop samples to introduce additional variability. Each instrument had a $30 \%$ chance of being excluded when creating the mixed loops, allowing for diverse instrument configurations in the dataset. To introduce further variation, instrumental loops were randomly sliced into four- or two-second segments. Pitch shifting was then applied using librosa [30] Python library, with bass loops shifted between -6 and $+ 2$ semitones, and other instruments shifted between -12 and $+ 1 2$ semitones.

Mixing and Mastering Simulation: To reduce the domain gap between the dataset and professionally produced music, we applied a series of digital audio effects during the loop generation process. In the mixing stage, each instrument and drum track was processed with gain adjustments, equalization (EQ), compression, panning, and limiting, with additional effects such as reverb and delay applied using parallel processing chains. This processing introduced sufficient variability in the audio characteristics, enhancing the dataset’s diversity to improve the model’s ability to generalize to real-world musical contexts.

During the mastering stage, the mixed audio was subjected to final EQ and limiting adjustments, producing variable outputs reflective of diverse audio environments. The parameters of these effects were randomized to ensure the dataset covered a broad range of production styles, reducing potential domain mismatch during inference. All audio files were exported in 16-bit, $4 4 . 1 ~ \mathrm { k H z }$ stereo WAV format to maintain consistency and compatibility with standard audio processing tools. All digital audio effects were implemented using the pedalboard [31] and pymixconsole [32] Python libraries.

V. EXPERIMENTS In this section, we describe the models under comparison and the datasets used for evaluation. We then report experimental results and analyses.

A. Compared Models We evaluate the following models: • DOSE: Our proposed generation-based method that bypasses source separation. We also conduct an ablation study by comparing the DOSE model trained with and without the onset loss, to assess its impact on generating high-fidelity one-shot samples.

B. Datasets We use three datasets to evaluate the above models: C. Metrics We use two objective metrics to evaluate the quality of extracted or generated drum one-shot samples: Frechet Audio Distance (FAD).´ FAD [9] measures the distance between two distributions of audio embeddings. In our experiments, we employ two embedding models : VGGish [34], CLAP [35]. Both versions compare embeddings from the generated audio against a reference distribution, derived from ground-truth drum one-shots in the test set.

Multi-Scale Spectral Similarity (MSS). MSS [36] evaluates how closely a generated audio sample matches a reference audio sample in terms of its time-frequency representation. To compute the multiscale spectral similarity (MSS), we first transform the generated and reference signals into spectrograms at multiple scales, specifically using FFT window lengths of 2048, 1024, 512, 256, 128, and 64. We then compute the mean squared error (MSE) between the spectrograms at each scale and aggregate them.

D. Results and Discussion Table I summarizes the performance of each model across RMOD, RMOD Drums-only, and the Groove MIDI Dataset. We report both MSS and FAD (with VGG and CLAP embeddings). The following key observations emerge: VI. CONCLUSION In this paper, we introduced the task of Drum One-Shot Extraction, aiming to generate drum one-shot samples from a given music mixture. To tackle this task, we proposed the Random Mixture One-Shot Dataset (RMOD), containing one million training samples and 10,000 validation and test samples, each consisting of a music mixture paired with the corresponding drum one-shot samples.

We also introduced Drum One-Shot Extractor (DOSE), a neural audio codec-based model designed to generate high-quality drum one-shots from complex music inputs. Using objective metrics such as Frechet Audio Distance (FAD) and Multi-Scale Spectral Loss, ´ DOSE outperformed the baseline model, LarsNet, across multiple datasets, demonstrating its ability to generate perceptually accurate drum sounds.

A limitation of this work is the lack of paired data from real commercial music, which we aim to address in future work. Additionally, we plan to extend this approach to other instruments, enabling the generation of high-quality one-shots for a broader range of instruments, further enhancing music production possibilities.

REFERENCES

---
## Equations
$$$$

$$$$

$$$$

---
## Tables
### Table 1
Table(html='', caption=None, page=3, table_id=None)
