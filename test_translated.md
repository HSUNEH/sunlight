DOSE: 드럼 원샷 추출을 위한 음악 믹스 Suntae Hwang1, Seonghyeon Kang1, Kyungsu Kim1, Semin Ahn2, Kyogu Lee1,3 초록—드럼 원샷 샘플은 사운드 디자인 및 전자 음악에서 특히 음악 제작에 필수적입니다. 이 논문은 음악 믹스에서 존재하는 드럼 원샷을 추출하는 작업인 드럼 원샷 추출(Drum One-Shot Extraction)을 소개합니다. 이를 촉진하기 위해, 우리 팀은 해당 드럼 원샷 샘플과 쌍을 이룬 대규모 무작위로 배열된 음악 믹스를 포함하는 랜덤 믹스 원샷 데이터셋(Random Mixture One-shot Dataset, RMOD)을 제안합니다. 우리가 제안하는 모델인 드럼 원샷 추출기(Drum One-Shot Extractor, DOSE)는 전통적인 소스 분리 단계를 우회하여 end-to-end 추출을 위해 신경 오디오 코덱 언어 모델을 활용합니다. 또한 드럼 원샷의 초기 트랜지언트를 정확하게 예측하도록 독려하는 새로운 onset loss를 소개하여 음색 특성을 포착하는 데 필수적입니다. 우리는 이 접근법을 기준선(source separation-based extraction method)과 비교합니다. Frechet Audio Distance (FAD)와 Multi-Scale Spectral loss (MSS)를 사용하여 평가한 결과, onset loss로 개선된 DOSE가 기준선을 초과하여 음악 믹스에서 보다 정확하고 높은 품질의 드럼 원샷을 제공함을 보여줍니다. 코드, 모델 체크포인트, 오디오 예제는 https://github.com/HSUNEH/DOSE 에서 확인할 수 있습니다. 색인 용어—드럼, 원샷, 음악 소스 분리, 신경 오디오-코덱, 생성 모델 I. 서론 드럼 사운드는 현대 음악 제작의 기본 구성 요소로, 전자 음악, 힙합, 팝 등 다양한 장르에서 중요한 역할을 합니다. 이러한 장르에서 음악 제작자는 개별 드럼 원샷 샘플을 시퀀싱하여 리드믹 패턴을 구성하며, 이를 통해 리듬의 음색 및 시간적 특성에 대한 정밀한 제어가 가능합니다. 드럼 사운드가 음악 제작에서 중요함에 따라, 최근 딥러닝 기술의 발전을 포함한 고급 기술을 이용하여 드럼 원샷 샘플을 합성하는 데 상당한 연구 관심이 모이고 있습니다 [1]–[5]. 이러한 새로운 접근법은 드럼 종류나 음향 특성과 같은 조건부 입력을 활용하여, 신호 처리 기술에 대한 광범위한 지식 없이도 직관적인 샘플 생성을 촉진하는 것을 목표로 하고 있습니다.

많은 실용적인 시나리오에서, 음악 프로듀서는 기존의 녹음을 가지고 리믹스, 커버 버전 또는 기타 프로덕션을 만들기 위해 음악 혼합물에서 고품질 드럼 샘플을 직접 추출해야 합니다. 우리는 이 작업을 드럼 원샷 추출(Drum One-Shot Extraction)이라고 정의합니다. 이 작업에 대한 전통적인 접근 방식은 음악 소스 분리 기술을 적용하여 드럼 트랙을 분리하고, 그 다음 이 작업의 일부는 한국 정부(MSIT)가 지원하는 한국연구재단(NRF) 보조금 [No. RS-2023-00219429, $5 0 \%$ ]과 한국 정부(MSIT)가 지원하는 정보통신기술진흥원(IITP) 보조금 [No. RS-2022-II220320, 2022-0-00320, 1:1 다중 모달 상호작용을 위한 크로스모달 대화 모델링에 대한 인공지능 연구, $4 0 \%$ ], [No. RS-2021-II212068, 인공지능 혁신 허브(서울대학교 인공지능 연구소), $5 \%$ ], [No. RS-2021-II211343, 인공지능 대학원 프로그램(서울대학교), $5 \%$ ]에 의해 지원되었습니다. 고립된 원샷 샘플을 포함하는 세그먼트를 식별하고 추출합니다. 우리는 이러한 유형의 방법을 “분리에 기반한 접근법(separation-based approach)”이라고 부릅니다. 그러나 이 접근 방식은 고립된 원샷 세그먼트의 식별을 보장할 수 없으며, 분리 알고리즘에 의존하기 때문에 인위적인 결과물을 도입하고 샘플 품질을 저하할 수 있습니다.

이러한 제한 사항을 해결하기 위해, 우리는 중간 분리 단계를 우회하고 입력 음악 혼합물에서 드럼 원샷 샘플을 직접 생성하는 새로운 생성 기반의 원샷 추출 접근 방식을 제안합니다. 우리의 방법인 DOSE는 최첨단 신경 오디오 코덱 언어 모델링의 발전을 활용하여 드럼 원샷의 엔드 투 엔드 생성을 수행합니다. DOSE는 각 드럼 유형(킥, 스네어, 하이햇)에 대해 별도의 디코더 전용 트랜스포머를 사용하여 모든 드럼 유형에 대해 단일 모델을 사용하는 것보다 더 나은 추출 성능을 달성합니다.

DOSE의 아키텍처는 MusicGen [6]의 아키텍처를 밀접하게 따르며, 신경 오디오 코덱과 디코더 전용 변환기의 동일한 핵심 구성 요소를 활용합니다. DAC [7] 인코더는 음악 혼합물과 원샷 오디오를 이산 음향 토큰으로 인코딩합니다. 그 후 변환기는 음악 혼합물의 음향 토큰에 조건화된 원샷 샘플의 음향 토큰을 자기 회귀 방식으로 생성하도록 훈련됩니다. 생성된 토큰은 DAC 디코더를 통해 파형 오디오로 디코딩됩니다.

DOSE의 훈련과 평가를 위해, 우리는 임의 혼합 일회성 데이터셋(Random Mixture One-shot Dataset, RMOD)을 도입합니다. RMOD는 합성 생성된 음악 혼합물과 그에 해당하는 드럼 일회성 샘플로 구성된 새로운 쌍 데이터셋입니다. RMOD는 드럼 트랙과 악기 트랙을 임의로 혼합하여 생성된 360,000 쌍의 혼합 오디오와 해당 드럼 일회성 샘플로 구성되어 있습니다. 드럼 트랙은 일회성 샘플을 사용하여 합성되었습니다.

우리는 원샷 드럼 샘플 추출 작업에 대한 DOSE 및 기준 방법의 포괄적인 정량적 평가를 수행했습니다. RMOD 외에도, 우리는 평가에서 Groove MIDI Dataset [8]을 활용하였으며, 이는 보다 현실적인 드럼 퍼포먼스를 제공합니다. 우리의 실험 결과는 DOSE가 기준 방법보다 우수하다는 것을 보여줍니다. 기준 방법은 LarsNet[citation]을 사용하여 구현된 분리 기반 접근 방식으로, Frechet Audio Distance (FAD) [9] 및 Multi-Scale Spectral Similarity (MSS) [10]을 포함한 다양한 객관적 지표에서 성능이 뛰어난 것으로 나타났습니다.

II. 관련 연구 드럼 원샷 생성. 드럼 원샷 샘플 생성을 위한 신경 오디오 합성 방법의 발전으로 상당한 주목을 받았다. 초기 접근법들은 잠재 공간 탐색과 드럼 소리를 위한 제어 가능 합성을 도입한 변분 오토인코더 (VAE) [11]와 생성적 적대 신경망 (GAN) [12]과 같은 모델을 활용했다. 이후의 연구들인 NeuroDrum [2], DrumGAN [4], 및 StyleWaveGAN [5]은 다양한 오디오 특징(예: 밝기, 폭음, 깊이)을 조건으로 함으로써 제어 가능성과 음색의 다양성을 더욱 향상시켰다.

점수 기반 확산 모델 [13]은 최근 오디오 합성을 위한 강력한 패러다임으로 떠오르며 유연한 샘플링 전략을 제공합니다. 예를 들어, CRASH [14]는 확률 미분 방정식(SDE)을 활용하여 제어 가능한 방식으로 고해상도 타악기 소리 $( 4 4 . 1 \mathrm { k H z } )$를 생성하고, GAN 기반 방법의 충실도와 일치하면서 클래스 혼합과 같은 기술을 가능하게 하여 “혼합” 소리를 생성합니다. 이러한 발전은 드럼/타악기 소리 생성에 대한 접근 방법의 폭이 넓어지고 있음을 보여주며, 음악 제작자에게 더 많은 상호작용적이고 세밀한 창의적 가능성을 제공합니다.

드럼 소스 분리. 전통적인 음악 소스 분리의 목표는 믹스를 개별 악기 스템으로 분리하는 것이다. 여기에는 드럼, 베이스, 보컬 등이 포함된다 [15]–[18]. 드럼 중심의 작업에 더 구체적으로, Mezza 외 [19]는 드럼 소스 분리라는 새로운 도전을 소개하였으며, 이는 드럼 트랙을 개별 스템(킥, 스네어, 하이햇 등)으로 분해하는 것을 목표로 한다. 그들이 제안한 방법인 LarsNet은 드럼 구성 요소 분리를 통해 각 드럼 유형을 개별적으로 추출할 수 있도록 한다.

신경 오디오 코덱 언어 모델링. SoundStream [20], EnCodec [21], DAC [7]와 같은 신경 오디오 코덱은 최소한의 지각 손실로 오디오를 불연속 토큰으로 압축할 수 있는 능력 덕분에 주목받고 있다. 이러한 토크나이저는 변환기 모델과 결합되었을 때 자기 회귀 오디오 생성 작업에 활용될 수 있다. MusicGen [6]과 MusicLM [22]는 코드 기반 언어 모델을 통해 고품질의 음악 생성을 보여주는 대표적인 사례이다.

III. 방법 우리는 복잡한 오디오 혼합물에서 드럼 원샷 사운드(킥, 스네어, 하이햇)를 추출하기 위해 설계된 심층 학습 모델 DOSE를 제안합니다. MusicGen [6]에서 영감을 받은 DOSE는 이산 오디오 표현을 처리하기 위해 디코더 전용 트랜스포머를 사용합니다. 정확한 순간 예측을 강조하기 위해, 우리는 훈련 중에 새로운 온셋 손실을 도입합니다.

A. 자기회귀 음향 토큰 생성 DOSE는 혼합 오디오 토큰에 조건부로 음향 토큰을 자기 회귀적으로 예측하여 드럼 원샷 사운드를 생성합니다. 이는 Descript Audio Codec (DAC) [7]를 사용하여 오디오 웨이브폼을 이산 토큰으로 변환하고, 드럼 원샷 토큰을 예측하며, 이를 웨이브폼으로 다시 디코딩하는 과정을 포함합니다.

DAC 프로세스는 모노 오디오 입력 $x \in \mathbb { R } ^ { T _ { \mathrm { i n } } \cdot f _ { s } }$ 를 처리하여 이를 이산 코드로 토큰화합니다. 여기서 $K$는 코드북의 수, $N$은 코드북의 크기이며, $f _ { c } \ll f _ { s }$는 코덱 프레임률입니다.

디코더 전용 트랜스포머는 혼합 오디오 토큰에 조건부로 드럼 원샷 토큰을 자기 회귀적으로 예측합니다. 그런 다음 DAC 디코더는 이러한 토큰을 다시 오디오 파형으로 변환합니다. DOSE는 각 드럼 유형(킥, 스네어 및 하이햇)에 대해 개별적으로 훈련되며, 이는 디코더 전용 언어 모델의 구조를 따르며 [6]에서 제안된 대로 인터리빙 지연 패턴을 사용합니다 [23].

B. 훈련 손실 우리의 훈련 목표는 두 가지 교차 엔트로피 손실을 결합합니다: (1) 전체 길이 교차 엔트로피, 드럼 원샷에서 예측된 모든 토큰에 대해 계산되고, (2) 시작 손실로, 인덱스 $t + k \le K + 1$인 토큰에 집중하여 공격 또는 과도 부분을 강조합니다. 여기서 $t$는 코덱 프레임 인덱스이고 $k$는 코드북 인덱스입니다. 이러한 과도 영역은 인지된 음색에 강한 영향을 미칩니다 [24].

훈련에 사용된 최종 손실은 전체 길이 손실과 시작 손실의 합입니다. 이러한 설정은 모델이 일시적인 영역에 집중하도록 편향시켜 생성된 원샷 샘플의 지각적 충실도를 향상시킵니다.

IV. 데이터셋 기존 드럼 사운드 데이터셋은 혼합 오디오 트랙과 해당 드럼 원샷 샘플(예: 킥, 스네어, 하이햇) 간에 적절한 정렬이 부족하기 때문에 원샷 드럼 추출에 적합하지 않습니다. 이러한 제한 사항은 모델이 드럼 스템 분리 작업에 비해 원샷 드럼 추출을 학습하는 데 어려움을 주며, 후자는 더 확립된 데이터셋이 존재합니다.

이 격차를 해소하기 위해, 우리는 랜덤 혼합 원샷 데이터셋(Random Mixture One-shot Dataset, RMOD)을 개발하였습니다. 이는 무수히 많은 쌍의 무작위로 혼합된 음악 루프와 해당되는 드럼 원샷 샘플을 포함하는 대규모 데이터셋입니다. 이러한 쌍은 원샷 드럼 추출 모델을 훈련하고 평가하는 기반으로 사용됩니다.

RMOD의 전체 데이터 생성 과정은 그림 3에 나타나 있습니다. 드럼 원샷 샘플을 먼저 사용하여 드럼 루프를 생성한 후, 이를 기타, 피아노, 베이스와 같은 악기 루프와 혼합하여 완전한 음악 혼합물을 형성하였습니다.

드럼 원샷 샘플을 위해 공개적으로 이용 가능한 데이터셋 [25]–[27]을 활용하였습니다. 악기 루프는 Logic Pro [28] 라이브러리에서 확보하였습니다. 총 3,375개의 킥 샘플, 1,801개의 스네어 샘플, 1,278개의 하이햇 샘플, 454개의 피아노 샘플, 1,161개의 기타 샘플, 1,782개의 베이스 샘플, 그리고 202개의 보컬 샘플을 수집하였습니다. 다음 섹션에서 설명할 증강 및 혼합 기법을 사용하여 훈련을 위해 무작위로 혼합된 음악 믹스와 해당 드럼 원샷 샘플의 쌍을 백만 개 생성하였으며, 추가로 검증 및 테스트를 위해 각각 10,000개의 쌍도 생성하였습니다.

RMOD 데이터셋은 전용 저장소를 통해 공개적으로 제공되며, 더 넓은 연구 커뮤니티의 사용을 돕기 위한 자세한 문서도 함께 제공됩니다.

A. 드럼 원샷 증강 RMOD 데이터셋의 다양성을 높이고 실제 제작 기법을 반영하기 위해, 우리는 드럼 레이어링을 데이터 증강 방법으로 적용했습니다. 두 개의 드럼 원샷 샘플을 임의로 선택하고, 각각의 진폭 가중치를 (0.8, 0.2), (0.7, 0.3), (0.6, 0.4)로 설정했습니다. 이 가중치는 약 (-1.94 dB, -13.98 dB), (-3.01 dB, -10.46 dB), (-4.44 dB, -7.96 dB)로서, 다양한 드럼 조합을 생성하는 가중합 접근 방식을 사용했습니다. 이 방법은 데이터셋을 효과적으로 확장하여 모델에 더 풍부하고 다양한 훈련 예제를 제공합니다.

B. 루프 생성 과정 대규모의 다양성 있는 데이터셋을 효율적으로 생성하기 위해, RMOD는 실제 음악 트랙의 구조를 정확히 모방하는 것을 목표로 하지 않습니다. 대신, 우리는 무작위 혼합 과정을 사용하여 4초 루프를 생성하였으며, 이는 다수의 훈련 샘플을 빠르게 생성할 수 있게 해줍니다.

MIDI 생성: 우리는 각 루프 내에서 드럼 소리의 타이밍과 배치를 제어하기 위해 MIDI(뮤지컬 인스트루먼트 디지털 인터페이스) 음표 시퀀스를 생성하는 것부터 시작했습니다. miditoolkit [29] 파이썬 라이브러리를 사용하여, 각 4초 루프는 1,920개의 동일 간격의 그리드 포인트로 나누어졌으며, 킥 드럼 소리는 무작위로 2회에서 4회 발생하고, 스네어 드럼은 2회에서 4회 발생하며, 하이햇은 14회에서 18회 발생했습니다. 이러한 타이밍과 배치에서의 무작위 변동성은 모델에 다양한 드럼 시퀀스를 제공하여 실세계 시나리오에 대한 일반화를 향상시켰습니다.

MIDI로부터 오디오 렌더링: MIDI 시퀀스가 생성된 후, 각 음표는 RMOD의 해당 드럼 원샷 샘플에 매핑되었습니다. 이전 샘플이 여전히 재생되고 있는 동안 동일한 드럼 클래스에 대해 새로운 발생이 발생하면, 이전 소리는 해당 클래스 내에서 겹침을 방지하기 위해 잘렸습니다. 이러한 동작은 빠른 하이햇 타격과 같은 실제 시나리오를 반영하며, 서로 다른 드럼 클래스의 겹치는 소음에는 영향을 미치지 않습니다. 악기 트랙의 경우, 기타, 피아노, 베이스 및 보컬 루프 샘플을 사용하여 추가 변동성을 도입했습니다. 각 악기는 혼합 루프를 생성할 때 제외될 확률이 $30 \%$로 설정되어 데이터세트에서 다양한 악기 구성을 허용합니다. 추가 변화를 도입하기 위해, 악기 루프는 무작위로 4초 또는 2초 세그먼트로 잘렸습니다. 그런 다음 pitch shifting이 librosa [30] Python 라이브러리를 사용하여 적용되었으며, 베이스 루프는 -6에서 $+ 2$ 반음 사이로 이동되고, 다른 악기는 -12에서 $+ 1 2$ 반음 사이로 이동되었습니다.

믹싱 및 마스터링 시뮬레이션: 데이터셋과 전문적으로 제작된 음악 간의 도메인 간격을 줄이기 위해 루프 생성 과정에서 일련의 디지털 오디오 효과를 적용했습니다. 믹싱 단계에서 각 악기와 드럼 트랙은 게인 조정, 이퀄라이제이션 (EQ), 압축, 패닝, 리미팅 등을 통해 처리되었으며, 평행 처리 체인을 사용하여 리버브 및 딜레이와 같은 추가 효과를 적용했습니다. 이러한 처리는 오디오 특성의 충분한 변동성을 도입하여 데이터셋의 다양성을 향상시켜 모델이 실제 음악적 맥락에 일반화할 수 있는 능력을 개선했습니다.

마스터링 단계에서 믹스된 오디오는 최종 EQ 및 리미팅 조정에 따라 다양한 오디오 환경을 반영하는 변수 출력을 생성했습니다. 이러한 효과의 매개변수는 데이터셋이 넓은 범위의 프로덕션 스타일을 포함하도록 무작위화되어 추론 중 도메인 불일치 가능성을 줄였습니다. 모든 오디오 파일은 일관성과 표준 오디오 처리 도구와의 호환성을 유지하기 위해 16비트, $4 4 . 1 ~ \mathrm { k H z }$ 스테레오 WAV 포맷으로 내보냈습니다. 모든 디지털 오디오 효과는 pedalboard [31] 및 pymixconsole [32] Python 라이브러리를 사용하여 구현되었습니다.

V. 실험 이 섹션에서는 비교되는 모델과 평가에 사용된 데이터셋을 설명합니다. 그런 다음 실험 결과와 분석을 보고합니다.

A. 비교된 모델들 우리는 다음 모델들을 평가합니다: • DOSE: 소스 분리 단계를 우회하는 우리의 제안된 생성 기반 방법입니다. 또한, onset 손실이 있는 경우와 없는 경우에 훈련된 DOSE 모델을 비교하여 고충실도의 일회성 샘플 생성을 위한 그 영향을 평가하는 아블레이션 연구를 수행합니다.

B. 데이터셋 우리는 위의 모델을 평가하기 위해 세 가지 데이터셋을 사용합니다: C. 메트릭 우리는 추출되거나 생성된 드럼 원샷 샘플의 품질을 평가하기 위해 두 가지 객관적인 메트릭을 사용합니다: Frechet 오디오 거리 (FAD). FAD [9]는 두 개의 오디오 임베딩 분포 간의 거리를 측정합니다. 우리의 실험에서는 두 가지 임베딩 모델인 VGGish [34]와 CLAP [35]를 사용합니다. 두 버전 모두 생성된 오디오의 임베딩을 테스트 세트의 실제 드럼 원샷으로부터 파생된 참조 분포와 비교합니다.

다중 스케일 스펙트럼 유사성 (MSS). MSS [36]는 생성된 오디오 샘플이 참조 오디오 샘플과 시간-주파수 표현 관점에서 얼마나 밀접하게 일치하는지를 평가합니다. 다중 스케일 스펙트럼 유사성 (MSS)을 계산하기 위해, 먼저 생성된 신호와 참조 신호를 여러 스케일의 스펙트로그램으로 변환합니다. 구체적으로는 FFT 창 길이를 2048, 1024, 512, 256, 128, 64로 설정하여 변환합니다. 그런 다음 각 스케일에서 스펙트로그램 간의 평균 제곱 오차 (MSE)를 계산하고 이를 집계합니다.

D. 결과 및 논의 표 I는 RMOD, RMOD 드럼 전용 및 Groove MIDI 데이터셋 전반의 각 모델 성능을 요약합니다. 우리는 MSS 및 FAD (VGG 및 CLAP 임베딩 포함)를 모두 보고합니다. 다음의 주요 관찰이 나옵니다: VI. 결론 본 논문에서는 주어진 음악 혼합물에서 드럼 원샷 샘플을 생성하는 것을 목표로 하는 드럼 원샷 추출 과제를 소개하였습니다. 이 과제를 해결하기 위해, 우리는 하나의 백만 개의 훈련 샘플과 10,000개의 검증 및 테스트 샘플을 포함하는 무작위 혼합 원샷 데이터셋(RMOD)을 제안하였으며, 각 샘플은 음악 혼합물과 해당 드럼 원샷 샘플 쌍으로 구성됩니다.

우리는 또한 Drum One-Shot Extractor (DOSE)를 소개하였습니다. DOSE는 복잡한 음악 입력으로부터 고품질의 드럼 원샷을 생성하기 위해 설계된 신경 오디오 코덱 기반 모델입니다. Frechet Audio Distance (FAD) 및 Multi-Scale Spectral Loss와 같은 객관적 지표를 사용한 결과, DOSE는 여러 데이터셋에서 기본 모델인 LarsNet을 초월하여 지각적으로 정확한 드럼 사운드를 생성할 수 있는 능력을 입증하였습니다.

이 작업의 한계는 실제 상업 음악에서 쌍 데이터가 부족하다는 점이며, 이는 향후 연구에서 해결할 예정입니다. 또한, 우리는 이 접근 방식을 다른 악기로 확장할 계획이며, 이를 통해 더 다양한 악기를 위한 고품질 원샷 생성이 가능해져 음악 제작 가능성을 더욱 향상시킬 것입니다.

참고문헌

---
## Equations
$$$$

$$$$

$$$$

---
## Tables
### Table 1
Table(html='', caption=None, page=3, table_id=None)
